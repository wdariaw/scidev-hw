{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vModw8OzcBs5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! python pretrain.py --train_data_path='wikisent2.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s_niaTTLqHu2",
    "outputId": "9eeae5f2-d7be-4b1e-9413-9539ac926fd2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extension horovod.torch has not been built: /home/user/conda/lib/python3.7/site-packages/horovod/torch/mpi_lib/_mpi_lib.cpython-37m-x86_64-linux-gnu.so not found\n",
      "If this is not expected, reinstall Horovod with HOROVOD_WITH_PYTORCH=1 to debug the build error.\n",
      "Warning! MPI libs are missing, but python applications are still avaiable.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "paris is the [MASK] of france.\n",
      "Teacher: paris is the capital of france. \n",
      "Student: paris is the university of france.\n",
      "\n",
      "the primary [MASK] of the united states is english.\n",
      "Teacher: the primary language of the united states is english. \n",
      "Student: the primary name of the united states is english.\n",
      "\n",
      "a baseball game consists of at least nine [MASK].\n",
      "Teacher: a baseball game consists of at least nine innings. \n",
      "Student: a baseball game consists of at least nine years.\n",
      "\n",
      "topology is a branch of [MASK] concerned with the properties of geometric objects that remain unchanged under continuous transformations.\n",
      "Teacher: topology is a branch of mathematics concerned with the properties of geometric objects that remain unchanged under continuous transformations. \n",
      "Student: topology is a branch of objects concerned with the properties of geometric objects that remain unchanged under continuous transformations.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! python mlm_inference.py --inference_data_path='inference_data.txt' --model_weights_path='version_24/checkpoints/epoch=7-step=217715.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7E5hFU9IqHu3",
    "outputId": "b7322a61-82b7-43f7-b5ad-c134c7b31e66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100% 28.0/28.0 [00:00<00:00, 168kB/s]\n",
      "vocab.txt: 100% 232k/232k [00:00<00:00, 8.43MB/s]\n",
      "tokenizer.json: 100% 466k/466k [00:00<00:00, 26.0MB/s]\n",
      "config.json: 100% 570/570 [00:00<00:00, 3.62MB/s]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /content/lightning_logs\n",
      "2024-01-12 17:29:32.529663: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-12 17:29:32.529729: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-12 17:29:32.531149: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-12 17:29:33.998260: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                          | Params\n",
      "--------------------------------------------------------\n",
      "0 | model | BertForSequenceClassification | 14.4 M\n",
      "--------------------------------------------------------\n",
      "14.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "14.4 M    Total params\n",
      "57.403    Total estimated model params size (MB)\n",
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Epoch 0: 100% 241/241 [01:17<00:00,  3.13it/s, v_num=0, train_loss_step=0.636]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0% 0/27 [00:00<?, ?it/s]       \u001b[A\n",
      "Validation DataLoader 0:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  74% 20/27 [00:02<00:00,  8.46it/s]\u001b[A\n",
      "Validation DataLoader 0: 100% 27/27 [00:03<00:00,  8.53it/s]\u001b[A\n",
      "Epoch 1:   0% 0/241 [00:00<?, ?it/s, v_num=0, train_loss_step=0.636, eval_loss=0.607, train_loss_epoch=0.615]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Epoch 1: 100% 241/241 [01:20<00:00,  3.00it/s, v_num=0, train_loss_step=0.638, eval_loss=0.607, train_loss_epoch=0.615]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0% 0/27 [00:00<?, ?it/s]       \u001b[A\n",
      "Validation DataLoader 0:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  74% 20/27 [00:02<00:00,  8.20it/s]\u001b[A\n",
      "Validation DataLoader 0: 100% 27/27 [00:03<00:00,  8.28it/s]\u001b[A\n",
      "Epoch 2:   0% 0/241 [00:00<?, ?it/s, v_num=0, train_loss_step=0.638, eval_loss=0.607, train_loss_epoch=0.608]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Epoch 2: 100% 241/241 [01:20<00:00,  2.99it/s, v_num=0, train_loss_step=0.639, eval_loss=0.607, train_loss_epoch=0.608]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0% 0/27 [00:00<?, ?it/s]       \u001b[A\n",
      "Validation DataLoader 0:   0% 0/27 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  74% 20/27 [00:02<00:00,  8.14it/s]\u001b[A\n",
      "Validation DataLoader 0: 100% 27/27 [00:03<00:00,  8.24it/s]\u001b[A\n",
      "Epoch 2: 100% 241/241 [01:24<00:00,  2.87it/s, v_num=0, train_loss_step=0.639, eval_loss=0.606, train_loss_epoch=0.607]`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "Epoch 2: 100% 241/241 [01:24<00:00,  2.85it/s, v_num=0, train_loss_step=0.639, eval_loss=0.606, train_loss_epoch=0.607]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Testing DataLoader 0: 100% 66/66 [00:07<00:00,  8.28it/s]\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│\u001b[36m \u001b[0m\u001b[36m         test_f1         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.813974916934967    \u001b[0m\u001b[35m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m     test_precision      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6912751793861389    \u001b[0m\u001b[35m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m       test_recall       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           1.0           \u001b[0m\u001b[35m \u001b[0m│\n",
      "└───────────────────────────┴───────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "! python finetune.py --glue_cls_task='cola' --student_weights_path='version_24/checkpoints/epoch=7-step=217715.ckpt' --epochs=3 --lr=1e-5 --batch_size=32 --finetune_distilled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m9DKiCEDqHu4",
    "outputId": "1e9b5c6b-bcdf-4df2-eb9c-710cb72ecba5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2024-01-12 17:35:06.001895: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-12 17:35:06.001958: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-12 17:35:06.003778: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-12 17:35:07.645863: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                          | Params\n",
      "--------------------------------------------------------\n",
      "0 | model | BertForSequenceClassification | 14.4 M\n",
      "--------------------------------------------------------\n",
      "14.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "14.4 M    Total params\n",
      "57.403    Total estimated model params size (MB)\n",
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Epoch 0: 100% 1895/1895 [10:34<00:00,  2.99it/s, v_num=1, train_loss_step=0.584]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0% 0/211 [00:00<?, ?it/s]      \u001b[A\n",
      "Validation DataLoader 0:   0% 0/211 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   9% 20/211 [00:02<00:23,  8.01it/s]\u001b[A\n",
      "Validation DataLoader 0:  19% 40/211 [00:04<00:21,  8.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  28% 60/211 [00:07<00:18,  8.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  38% 80/211 [00:09<00:16,  8.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  47% 100/211 [00:12<00:13,  8.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  57% 120/211 [00:14<00:11,  8.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  66% 140/211 [00:17<00:08,  8.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  76% 160/211 [00:19<00:06,  8.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  85% 180/211 [00:22<00:03,  8.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  95% 200/211 [00:24<00:01,  8.14it/s]\u001b[A\n",
      "Validation DataLoader 0: 100% 211/211 [00:25<00:00,  8.16it/s]\u001b[A\n",
      "Epoch 1:   0% 0/1895 [00:00<?, ?it/s, v_num=1, train_loss_step=0.584, eval_loss=0.319, train_loss_epoch=0.437]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Epoch 1: 100% 1895/1895 [10:34<00:00,  2.99it/s, v_num=1, train_loss_step=0.527, eval_loss=0.319, train_loss_epoch=0.437]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0% 0/211 [00:00<?, ?it/s]      \u001b[A\n",
      "Validation DataLoader 0:   0% 0/211 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   9% 20/211 [00:02<00:23,  8.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  19% 40/211 [00:04<00:21,  8.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  28% 60/211 [00:07<00:18,  8.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  38% 80/211 [00:09<00:16,  8.18it/s]\u001b[A\n",
      "Validation DataLoader 0:  47% 100/211 [00:12<00:13,  8.19it/s]\u001b[A\n",
      "Validation DataLoader 0:  57% 120/211 [00:14<00:11,  8.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  66% 140/211 [00:17<00:08,  8.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  76% 160/211 [00:19<00:06,  8.15it/s]\u001b[A\n",
      "Validation DataLoader 0:  85% 180/211 [00:22<00:03,  8.16it/s]\u001b[A\n",
      "Validation DataLoader 0:  95% 200/211 [00:24<00:01,  8.16it/s]\u001b[A\n",
      "Validation DataLoader 0: 100% 211/211 [00:25<00:00,  8.19it/s]\u001b[A\n",
      "Epoch 2:   0% 0/1895 [00:00<?, ?it/s, v_num=1, train_loss_step=0.527, eval_loss=0.261, train_loss_epoch=0.308]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Epoch 2: 100% 1895/1895 [10:34<00:00,  2.99it/s, v_num=1, train_loss_step=0.428, eval_loss=0.261, train_loss_epoch=0.308]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0% 0/211 [00:00<?, ?it/s]      \u001b[A\n",
      "Validation DataLoader 0:   0% 0/211 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   9% 20/211 [00:02<00:23,  8.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  19% 40/211 [00:04<00:21,  8.02it/s]\u001b[A\n",
      "Validation DataLoader 0:  28% 60/211 [00:07<00:18,  8.04it/s]\u001b[A\n",
      "Validation DataLoader 0:  38% 80/211 [00:09<00:16,  8.09it/s]\u001b[A\n",
      "Validation DataLoader 0:  47% 100/211 [00:12<00:13,  8.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  57% 120/211 [00:14<00:11,  8.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  66% 140/211 [00:17<00:08,  8.14it/s]\u001b[A\n",
      "Validation DataLoader 0:  76% 160/211 [00:19<00:06,  8.13it/s]\u001b[A\n",
      "Validation DataLoader 0:  85% 180/211 [00:22<00:03,  8.12it/s]\u001b[A\n",
      "Validation DataLoader 0:  95% 200/211 [00:24<00:01,  8.13it/s]\u001b[A\n",
      "Validation DataLoader 0: 100% 211/211 [00:25<00:00,  8.16it/s]\u001b[A\n",
      "Epoch 2: 100% 1895/1895 [11:00<00:00,  2.87it/s, v_num=1, train_loss_step=0.428, eval_loss=0.226, train_loss_epoch=0.253]`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "Epoch 2: 100% 1895/1895 [11:01<00:00,  2.87it/s, v_num=1, train_loss_step=0.428, eval_loss=0.226, train_loss_epoch=0.253]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Testing DataLoader 0: 100% 55/55 [00:06<00:00,  8.20it/s]\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│\u001b[36m \u001b[0m\u001b[36m         test_f1         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8290132880210876    \u001b[0m\u001b[35m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m     test_precision      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8479633927345276    \u001b[0m\u001b[35m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m       test_recall       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8187316060066223    \u001b[0m\u001b[35m \u001b[0m│\n",
      "└───────────────────────────┴───────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "! python finetune.py --glue_cls_task='sst2' --student_weights_path='version_24/checkpoints/epoch=7-step=217715.ckpt' --epochs=3 --lr=1e-5 --batch_size=32 --finetune_distilled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nh_FHwY_qHu4"
   },
   "source": [
    "Смотрим распределение классов в тестовом датасете cola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EyJpkJK4qHu6",
    "outputId": "d68ed2c6-14c8-4e62-e0f6-e27ad38e7ff3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "from data_modules import GLUEDataModule\n",
    "from transformers import BertTokenizer\n",
    "from utils import seed_everything\n",
    "\n",
    "seed_everything(42)\n",
    "params = Namespace()\n",
    "params.max_length = 512\n",
    "params.model_name = \"bert-base-uncased\"\n",
    "params.glue_cls_task = 'cola'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(params.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "cG_IfqvOiOyo"
   },
   "outputs": [],
   "source": [
    "data_module = GLUEDataModule(params, tokenizer)\n",
    "data_module.prepare_data()\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "SAa-cC-BqHu7"
   },
   "outputs": [],
   "source": [
    "test_labels = []\n",
    "\n",
    "for x in data_module.test_dataset:\n",
    "    test_labels.append(x['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w4y7-P_oqHu7",
    "outputId": "5e4c576c-e786-497c-8008-148d23748a07"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3087, 0.6913])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(test_labels) / len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wt1L4Sq8qHu8"
   },
   "source": [
    "Дообученная на COLA модель всегда предсказывают класс 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOE3KRyzqHu9"
   },
   "source": [
    "Дообучаем bert_base_uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-_fBKW8rqHu9",
    "outputId": "59767a89-3fbf-40de-cb6c-f783a2ad8286"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2024-01-12 18:31:48.986454: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-12 18:31:48.986519: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-12 18:31:48.988528: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-12 18:31:50.493732: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                          | Params\n",
      "--------------------------------------------------------\n",
      "0 | model | BertForSequenceClassification | 109 M \n",
      "--------------------------------------------------------\n",
      "109 M     Trainable params\n",
      "0         Non-trainable params\n",
      "109 M     Total params\n",
      "437.935   Total estimated model params size (MB)\n",
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Epoch 0: 100% 962/962 [11:28<00:00,  1.40it/s, v_num=4, train_loss_step=0.360]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0% 0/107 [00:00<?, ?it/s]      \u001b[A\n",
      "Validation DataLoader 0:   0% 0/107 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  19% 20/107 [00:05<00:22,  3.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  37% 40/107 [00:10<00:17,  3.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  56% 60/107 [00:15<00:12,  3.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  75% 80/107 [00:20<00:07,  3.83it/s]\u001b[A\n",
      "Validation DataLoader 0:  93% 100/107 [00:26<00:01,  3.83it/s]\u001b[A\n",
      "Validation DataLoader 0: 100% 107/107 [00:27<00:00,  3.83it/s]\u001b[A\n",
      "Epoch 1:   0% 0/962 [00:00<?, ?it/s, v_num=4, train_loss_step=0.360, eval_loss=0.443, train_loss_epoch=0.523]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Epoch 1: 100% 962/962 [11:35<00:00,  1.38it/s, v_num=4, train_loss_step=0.0709, eval_loss=0.443, train_loss_epoch=0.523]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0% 0/107 [00:00<?, ?it/s]      \u001b[A\n",
      "Validation DataLoader 0:   0% 0/107 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  19% 20/107 [00:05<00:22,  3.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  37% 40/107 [00:10<00:17,  3.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  56% 60/107 [00:15<00:12,  3.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  75% 80/107 [00:20<00:07,  3.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  93% 100/107 [00:26<00:01,  3.84it/s]\u001b[A\n",
      "Validation DataLoader 0: 100% 107/107 [00:27<00:00,  3.84it/s]\u001b[A\n",
      "Epoch 2:   0% 0/962 [00:00<?, ?it/s, v_num=4, train_loss_step=0.0709, eval_loss=0.522, train_loss_epoch=0.332]/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Epoch 2: 100% 962/962 [11:35<00:00,  1.38it/s, v_num=4, train_loss_step=0.0808, eval_loss=0.522, train_loss_epoch=0.332]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0% 0/107 [00:00<?, ?it/s]      \u001b[A\n",
      "Validation DataLoader 0:   0% 0/107 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  19% 20/107 [00:05<00:22,  3.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  37% 40/107 [00:10<00:17,  3.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  56% 60/107 [00:15<00:12,  3.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  75% 80/107 [00:20<00:07,  3.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  93% 100/107 [00:26<00:01,  3.83it/s]\u001b[A\n",
      "Validation DataLoader 0: 100% 107/107 [00:27<00:00,  3.83it/s]\u001b[A\n",
      "Epoch 2: 100% 962/962 [12:03<00:00,  1.33it/s, v_num=4, train_loss_step=0.0808, eval_loss=0.555, train_loss_epoch=0.209]`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "Epoch 2: 100% 962/962 [12:09<00:00,  1.32it/s, v_num=4, train_loss_step=0.0808, eval_loss=0.555, train_loss_epoch=0.209]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Testing DataLoader 0: 100% 261/261 [01:09<00:00,  3.77it/s]\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│\u001b[36m \u001b[0m\u001b[36m         test_f1         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8731679320335388    \u001b[0m\u001b[35m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m     test_precision      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8312879800796509    \u001b[0m\u001b[35m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m       test_recall       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9455280900001526    \u001b[0m\u001b[35m \u001b[0m│\n",
      "└───────────────────────────┴───────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "! python finetune.py --glue_cls_task='cola' --epochs=3 --lr=1e-5 --batch_size=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fmZqwd9qqHu9",
    "outputId": "6ce686f4-44fe-4a8f-f30b-7f26b0c23108",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "libgomp: Invalid value for environment variable OMP_NUM_THREADS\n",
      "Extension horovod.torch has not been built: /home/user/conda/lib/python3.7/site-packages/horovod/torch/mpi_lib/_mpi_lib.cpython-37m-x86_64-linux-gnu.so not found\n",
      "If this is not expected, reinstall Horovod with HOROVOD_WITH_PYTORCH=1 to debug the build error.\n",
      "Warning! MPI libs are missing, but python applications are still avaiable.\n",
      "\n",
      "libgomp: Invalid value for environment variable OMP_NUM_THREADS\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Loading cached split indices for dataset at /home/jovyan/GigaChat/censor_dasha/sst2.hf/train/cache-8d67fbfceca87106.arrow and /home/jovyan/GigaChat/censor_dasha/sst2.hf/train/cache-b41b7446ba1d3037.arrow\n",
      "You are using a CUDA device ('A100-PCIE-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                          | Params\n",
      "--------------------------------------------------------\n",
      "0 | model | BertForSequenceClassification | 109 M \n",
      "--------------------------------------------------------\n",
      "109 M     Trainable params\n",
      "0         Non-trainable params\n",
      "109 M     Total params\n",
      "437.935   Total estimated model params size (MB)\n",
      "/home/jovyan/.imgenv-cloud-a100-gigachat-0/lib/python3.7/site-packages/torch/utils/tensorboard/__init__.py:5: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  tensorboard.__version__\n",
      "/home/user/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:572: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  (np.object, string),\n",
      "/home/user/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:573: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  (np.bool, bool),\n",
      "/home/user/conda/lib/python3.7/site-packages/tensorboard/util/tensor_util.py:113: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.object: SlowAppendObjectArrayToTensorProto,\n",
      "/home/user/conda/lib/python3.7/site-packages/tensorboard/util/tensor_util.py:114: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.bool: SlowAppendBoolArrayToTensorProto,\n",
      "/home/user/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:585: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.object,\n",
      "/home/user/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:637: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.bool,\n",
      "/home/user/conda/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:108: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.object:\n",
      "/home/user/conda/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:110: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.bool:\n",
      "/home/user/conda/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py:22: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "/home/user/conda/lib/python3.7/site-packages/tensorflow/python/ops/numpy_ops/np_random.py:110: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  def randint(low, high=None, size=None, dtype=onp.int):  # pylint: disable=missing-function-docstring\n",
      "Epoch 0:  90%|▉| 1895/2106 [22:26<02:29,  1.41it/s, loss=0.172, v_num=1, train_l\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/211 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/211 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  90%|▉| 1896/2106 [22:27<02:29,  1.41it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  90%|▉| 1897/2106 [22:27<02:28,  1.41it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  90%|▉| 1898/2106 [22:27<02:27,  1.41it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  90%|▉| 1899/2106 [22:27<02:26,  1.41it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  90%|▉| 1900/2106 [22:27<02:26,  1.41it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  90%|▉| 1901/2106 [22:28<02:25,  1.41it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  90%|▉| 1902/2106 [22:28<02:24,  1.41it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  90%|▉| 1903/2106 [22:28<02:23,  1.41it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  90%|▉| 1904/2106 [22:28<02:23,  1.41it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  90%|▉| 1905/2106 [22:29<02:22,  1.41it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  91%|▉| 1906/2106 [22:29<02:21,  1.41it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  91%|▉| 1907/2106 [22:29<02:20,  1.41it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  91%|▉| 1908/2106 [22:29<02:20,  1.41it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  91%|▉| 1909/2106 [22:30<02:19,  1.41it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  91%|▉| 1910/2106 [22:30<02:18,  1.41it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  91%|▉| 1911/2106 [22:30<02:17,  1.41it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  91%|▉| 1912/2106 [22:30<02:17,  1.42it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  91%|▉| 1913/2106 [22:31<02:16,  1.42it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  91%|▉| 1914/2106 [22:31<02:15,  1.42it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  91%|▉| 1915/2106 [22:31<02:14,  1.42it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  91%|▉| 1916/2106 [22:31<02:14,  1.42it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  91%|▉| 1917/2106 [22:31<02:13,  1.42it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  91%|▉| 1918/2106 [22:32<02:12,  1.42it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  91%|▉| 1919/2106 [22:32<02:11,  1.42it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  91%|▉| 1920/2106 [22:32<02:11,  1.42it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  91%|▉| 1921/2106 [22:32<02:10,  1.42it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  91%|▉| 1922/2106 [22:33<02:09,  1.42it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  91%|▉| 1923/2106 [22:33<02:08,  1.42it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  91%|▉| 1924/2106 [22:33<02:08,  1.42it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  91%|▉| 1925/2106 [22:33<02:07,  1.42it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  91%|▉| 1926/2106 [22:34<02:06,  1.42it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  92%|▉| 1927/2106 [22:34<02:05,  1.42it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  92%|▉| 1928/2106 [22:34<02:05,  1.42it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  92%|▉| 1929/2106 [22:34<02:04,  1.42it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  92%|▉| 1930/2106 [22:35<02:03,  1.42it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  92%|▉| 1931/2106 [22:35<02:02,  1.42it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  92%|▉| 1932/2106 [22:35<02:02,  1.43it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  92%|▉| 1933/2106 [22:35<02:01,  1.43it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  92%|▉| 1934/2106 [22:36<02:00,  1.43it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  92%|▉| 1935/2106 [22:36<01:59,  1.43it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  92%|▉| 1936/2106 [22:36<01:59,  1.43it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  92%|▉| 1937/2106 [22:36<01:58,  1.43it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  92%|▉| 1938/2106 [22:36<01:57,  1.43it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  92%|▉| 1939/2106 [22:37<01:56,  1.43it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  92%|▉| 1940/2106 [22:37<01:56,  1.43it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  92%|▉| 1941/2106 [22:37<01:55,  1.43it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  92%|▉| 1942/2106 [22:37<01:54,  1.43it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  92%|▉| 1943/2106 [22:38<01:53,  1.43it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  92%|▉| 1944/2106 [22:38<01:53,  1.43it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  92%|▉| 1945/2106 [22:38<01:52,  1.43it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  92%|▉| 1946/2106 [22:38<01:51,  1.43it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  92%|▉| 1947/2106 [22:39<01:50,  1.43it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  92%|▉| 1948/2106 [22:39<01:50,  1.43it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  93%|▉| 1949/2106 [22:39<01:49,  1.43it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  93%|▉| 1950/2106 [22:39<01:48,  1.43it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  93%|▉| 1951/2106 [22:40<01:48,  1.43it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  93%|▉| 1952/2106 [22:40<01:47,  1.44it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  93%|▉| 1953/2106 [22:40<01:46,  1.44it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  93%|▉| 1954/2106 [22:40<01:45,  1.44it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  93%|▉| 1955/2106 [22:40<01:45,  1.44it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  93%|▉| 1956/2106 [22:41<01:44,  1.44it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  93%|▉| 1957/2106 [22:41<01:43,  1.44it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  93%|▉| 1958/2106 [22:41<01:42,  1.44it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  93%|▉| 1959/2106 [22:41<01:42,  1.44it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  93%|▉| 1960/2106 [22:42<01:41,  1.44it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  93%|▉| 1961/2106 [22:42<01:40,  1.44it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  93%|▉| 1962/2106 [22:42<01:40,  1.44it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  93%|▉| 1963/2106 [22:42<01:39,  1.44it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  93%|▉| 1964/2106 [22:43<01:38,  1.44it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  93%|▉| 1965/2106 [22:43<01:37,  1.44it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  93%|▉| 1966/2106 [22:43<01:37,  1.44it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  93%|▉| 1967/2106 [22:43<01:36,  1.44it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  93%|▉| 1968/2106 [22:44<01:35,  1.44it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  93%|▉| 1969/2106 [22:44<01:34,  1.44it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  94%|▉| 1970/2106 [22:44<01:34,  1.44it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  94%|▉| 1971/2106 [22:44<01:33,  1.44it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  94%|▉| 1972/2106 [22:44<01:32,  1.44it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  94%|▉| 1973/2106 [22:45<01:32,  1.45it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  94%|▉| 1974/2106 [22:45<01:31,  1.45it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  94%|▉| 1975/2106 [22:45<01:30,  1.45it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  94%|▉| 1976/2106 [22:45<01:29,  1.45it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  94%|▉| 1977/2106 [22:46<01:29,  1.45it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  94%|▉| 1978/2106 [22:46<01:28,  1.45it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  94%|▉| 1979/2106 [22:46<01:27,  1.45it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  94%|▉| 1980/2106 [22:46<01:26,  1.45it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  94%|▉| 1981/2106 [22:47<01:26,  1.45it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  94%|▉| 1982/2106 [22:47<01:25,  1.45it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  94%|▉| 1983/2106 [22:47<01:24,  1.45it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  94%|▉| 1984/2106 [22:47<01:24,  1.45it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  94%|▉| 1985/2106 [22:48<01:23,  1.45it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  94%|▉| 1986/2106 [22:48<01:22,  1.45it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  94%|▉| 1987/2106 [22:48<01:21,  1.45it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  94%|▉| 1988/2106 [22:48<01:21,  1.45it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  94%|▉| 1989/2106 [22:49<01:20,  1.45it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  94%|▉| 1990/2106 [22:49<01:19,  1.45it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  95%|▉| 1991/2106 [22:49<01:19,  1.45it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  95%|▉| 1992/2106 [22:49<01:18,  1.45it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  95%|▉| 1993/2106 [22:49<01:17,  1.45it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  95%|▉| 1994/2106 [22:50<01:16,  1.46it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  95%|▉| 1995/2106 [22:50<01:16,  1.46it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  95%|▉| 1996/2106 [22:50<01:15,  1.46it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  95%|▉| 1997/2106 [22:50<01:14,  1.46it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  95%|▉| 1998/2106 [22:51<01:14,  1.46it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  95%|▉| 1999/2106 [22:51<01:13,  1.46it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  95%|▉| 2000/2106 [22:51<01:12,  1.46it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  95%|▉| 2001/2106 [22:51<01:11,  1.46it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  95%|▉| 2002/2106 [22:52<01:11,  1.46it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  95%|▉| 2003/2106 [22:52<01:10,  1.46it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  95%|▉| 2004/2106 [22:52<01:09,  1.46it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  95%|▉| 2005/2106 [22:52<01:09,  1.46it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  95%|▉| 2006/2106 [22:53<01:08,  1.46it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  95%|▉| 2007/2106 [22:53<01:07,  1.46it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  95%|▉| 2008/2106 [22:53<01:07,  1.46it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  95%|▉| 2009/2106 [22:53<01:06,  1.46it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  95%|▉| 2010/2106 [22:53<01:05,  1.46it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  95%|▉| 2011/2106 [22:54<01:04,  1.46it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  96%|▉| 2012/2106 [22:54<01:04,  1.46it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  96%|▉| 2013/2106 [22:54<01:03,  1.46it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  96%|▉| 2014/2106 [22:54<01:02,  1.46it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  96%|▉| 2015/2106 [22:55<01:02,  1.47it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  96%|▉| 2016/2106 [22:55<01:01,  1.47it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  96%|▉| 2017/2106 [22:55<01:00,  1.47it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  96%|▉| 2018/2106 [22:55<00:59,  1.47it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  96%|▉| 2019/2106 [22:56<00:59,  1.47it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  96%|▉| 2020/2106 [22:56<00:58,  1.47it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  96%|▉| 2021/2106 [22:56<00:57,  1.47it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  96%|▉| 2022/2106 [22:56<00:57,  1.47it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  96%|▉| 2023/2106 [22:57<00:56,  1.47it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  96%|▉| 2024/2106 [22:57<00:55,  1.47it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  96%|▉| 2025/2106 [22:57<00:55,  1.47it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  96%|▉| 2026/2106 [22:57<00:54,  1.47it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  96%|▉| 2027/2106 [22:57<00:53,  1.47it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  96%|▉| 2028/2106 [22:58<00:53,  1.47it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  96%|▉| 2029/2106 [22:58<00:52,  1.47it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  96%|▉| 2030/2106 [22:58<00:51,  1.47it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  96%|▉| 2031/2106 [22:58<00:50,  1.47it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  96%|▉| 2032/2106 [22:59<00:50,  1.47it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  97%|▉| 2033/2106 [22:59<00:49,  1.47it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  97%|▉| 2034/2106 [22:59<00:48,  1.47it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  97%|▉| 2035/2106 [22:59<00:48,  1.47it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  97%|▉| 2036/2106 [23:00<00:47,  1.48it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  97%|▉| 2037/2106 [23:00<00:46,  1.48it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  97%|▉| 2038/2106 [23:00<00:46,  1.48it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  97%|▉| 2039/2106 [23:00<00:45,  1.48it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  97%|▉| 2040/2106 [23:01<00:44,  1.48it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  97%|▉| 2041/2106 [23:01<00:43,  1.48it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  97%|▉| 2042/2106 [23:01<00:43,  1.48it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  97%|▉| 2043/2106 [23:01<00:42,  1.48it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  97%|▉| 2044/2106 [23:02<00:41,  1.48it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  97%|▉| 2045/2106 [23:02<00:41,  1.48it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  97%|▉| 2046/2106 [23:02<00:40,  1.48it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  97%|▉| 2047/2106 [23:02<00:39,  1.48it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  97%|▉| 2048/2106 [23:02<00:39,  1.48it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  97%|▉| 2049/2106 [23:03<00:38,  1.48it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  97%|▉| 2050/2106 [23:03<00:37,  1.48it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  97%|▉| 2051/2106 [23:03<00:37,  1.48it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  97%|▉| 2052/2106 [23:03<00:36,  1.48it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  97%|▉| 2053/2106 [23:04<00:35,  1.48it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  98%|▉| 2054/2106 [23:04<00:35,  1.48it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  98%|▉| 2055/2106 [23:04<00:34,  1.48it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  98%|▉| 2056/2106 [23:04<00:33,  1.48it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  98%|▉| 2057/2106 [23:05<00:32,  1.49it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  98%|▉| 2058/2106 [23:05<00:32,  1.49it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  98%|▉| 2059/2106 [23:05<00:31,  1.49it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  98%|▉| 2060/2106 [23:05<00:30,  1.49it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  98%|▉| 2061/2106 [23:06<00:30,  1.49it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  98%|▉| 2062/2106 [23:06<00:29,  1.49it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  98%|▉| 2063/2106 [23:06<00:28,  1.49it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  98%|▉| 2064/2106 [23:06<00:28,  1.49it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  98%|▉| 2065/2106 [23:06<00:27,  1.49it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  98%|▉| 2066/2106 [23:07<00:26,  1.49it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  98%|▉| 2067/2106 [23:07<00:26,  1.49it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  98%|▉| 2068/2106 [23:07<00:25,  1.49it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  98%|▉| 2069/2106 [23:07<00:24,  1.49it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  98%|▉| 2070/2106 [23:08<00:24,  1.49it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  98%|▉| 2071/2106 [23:08<00:23,  1.49it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  98%|▉| 2072/2106 [23:08<00:22,  1.49it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  98%|▉| 2073/2106 [23:08<00:22,  1.49it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  98%|▉| 2074/2106 [23:09<00:21,  1.49it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  99%|▉| 2075/2106 [23:09<00:20,  1.49it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  99%|▉| 2076/2106 [23:09<00:20,  1.49it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  99%|▉| 2077/2106 [23:09<00:19,  1.49it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  99%|▉| 2078/2106 [23:10<00:18,  1.49it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  99%|▉| 2079/2106 [23:10<00:18,  1.50it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  99%|▉| 2080/2106 [23:10<00:17,  1.50it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  99%|▉| 2081/2106 [23:10<00:16,  1.50it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  99%|▉| 2082/2106 [23:11<00:16,  1.50it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  99%|▉| 2083/2106 [23:11<00:15,  1.50it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  99%|▉| 2084/2106 [23:11<00:14,  1.50it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  99%|▉| 2085/2106 [23:11<00:14,  1.50it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  99%|▉| 2086/2106 [23:11<00:13,  1.50it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  99%|▉| 2087/2106 [23:12<00:12,  1.50it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  99%|▉| 2088/2106 [23:12<00:12,  1.50it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  99%|▉| 2089/2106 [23:12<00:11,  1.50it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  99%|▉| 2090/2106 [23:12<00:10,  1.50it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  99%|▉| 2091/2106 [23:13<00:09,  1.50it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  99%|▉| 2092/2106 [23:13<00:09,  1.50it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  99%|▉| 2093/2106 [23:13<00:08,  1.50it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  99%|▉| 2094/2106 [23:13<00:07,  1.50it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0:  99%|▉| 2095/2106 [23:14<00:07,  1.50it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0: 100%|▉| 2096/2106 [23:14<00:06,  1.50it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0: 100%|▉| 2097/2106 [23:14<00:05,  1.50it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0: 100%|▉| 2098/2106 [23:14<00:05,  1.50it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0: 100%|▉| 2099/2106 [23:15<00:04,  1.50it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0: 100%|▉| 2100/2106 [23:15<00:03,  1.51it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0: 100%|▉| 2101/2106 [23:15<00:03,  1.51it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0: 100%|▉| 2102/2106 [23:15<00:02,  1.51it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0: 100%|▉| 2103/2106 [23:15<00:01,  1.51it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0: 100%|▉| 2104/2106 [23:16<00:01,  1.51it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0: 100%|▉| 2105/2106 [23:16<00:00,  1.51it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 0: 100%|█| 2106/2106 [23:16<00:00,  1.51it/s, loss=0.172, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  90%|▉| 1895/2106 [22:26<02:29,  1.41it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/211 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/211 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  90%|▉| 1896/2106 [22:26<02:29,  1.41it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  90%|▉| 1897/2106 [22:27<02:28,  1.41it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  90%|▉| 1898/2106 [22:27<02:27,  1.41it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  90%|▉| 1899/2106 [22:27<02:26,  1.41it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  90%|▉| 1900/2106 [22:27<02:26,  1.41it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  90%|▉| 1901/2106 [22:27<02:25,  1.41it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  90%|▉| 1902/2106 [22:28<02:24,  1.41it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  90%|▉| 1903/2106 [22:28<02:23,  1.41it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  90%|▉| 1904/2106 [22:28<02:23,  1.41it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  90%|▉| 1905/2106 [22:28<02:22,  1.41it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  91%|▉| 1906/2106 [22:29<02:21,  1.41it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  91%|▉| 1907/2106 [22:29<02:20,  1.41it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  91%|▉| 1908/2106 [22:29<02:20,  1.41it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  91%|▉| 1909/2106 [22:29<02:19,  1.41it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  91%|▉| 1910/2106 [22:30<02:18,  1.41it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  91%|▉| 1911/2106 [22:30<02:17,  1.42it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  91%|▉| 1912/2106 [22:30<02:17,  1.42it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  91%|▉| 1913/2106 [22:30<02:16,  1.42it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  91%|▉| 1914/2106 [22:31<02:15,  1.42it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  91%|▉| 1915/2106 [22:31<02:14,  1.42it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  91%|▉| 1916/2106 [22:31<02:14,  1.42it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  91%|▉| 1917/2106 [22:31<02:13,  1.42it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  91%|▉| 1918/2106 [22:31<02:12,  1.42it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  91%|▉| 1919/2106 [22:32<02:11,  1.42it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  91%|▉| 1920/2106 [22:32<02:11,  1.42it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  91%|▉| 1921/2106 [22:32<02:10,  1.42it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  91%|▉| 1922/2106 [22:32<02:09,  1.42it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  91%|▉| 1923/2106 [22:33<02:08,  1.42it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  91%|▉| 1924/2106 [22:33<02:08,  1.42it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  91%|▉| 1925/2106 [22:33<02:07,  1.42it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  91%|▉| 1926/2106 [22:33<02:06,  1.42it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  92%|▉| 1927/2106 [22:34<02:05,  1.42it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  92%|▉| 1928/2106 [22:34<02:05,  1.42it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  92%|▉| 1929/2106 [22:34<02:04,  1.42it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  92%|▉| 1930/2106 [22:34<02:03,  1.42it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  92%|▉| 1931/2106 [22:35<02:02,  1.43it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  92%|▉| 1932/2106 [22:35<02:02,  1.43it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  92%|▉| 1933/2106 [22:35<02:01,  1.43it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  92%|▉| 1934/2106 [22:35<02:00,  1.43it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  92%|▉| 1935/2106 [22:36<01:59,  1.43it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  92%|▉| 1936/2106 [22:36<01:59,  1.43it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  92%|▉| 1937/2106 [22:36<01:58,  1.43it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  92%|▉| 1938/2106 [22:36<01:57,  1.43it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  92%|▉| 1939/2106 [22:36<01:56,  1.43it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  92%|▉| 1940/2106 [22:37<01:56,  1.43it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  92%|▉| 1941/2106 [22:37<01:55,  1.43it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  92%|▉| 1942/2106 [22:37<01:54,  1.43it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  92%|▉| 1943/2106 [22:37<01:53,  1.43it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  92%|▉| 1944/2106 [22:38<01:53,  1.43it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  92%|▉| 1945/2106 [22:38<01:52,  1.43it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  92%|▉| 1946/2106 [22:38<01:51,  1.43it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  92%|▉| 1947/2106 [22:38<01:50,  1.43it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  92%|▉| 1948/2106 [22:39<01:50,  1.43it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  93%|▉| 1949/2106 [22:39<01:49,  1.43it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  93%|▉| 1950/2106 [22:39<01:48,  1.43it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  93%|▉| 1951/2106 [22:39<01:48,  1.43it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  93%|▉| 1952/2106 [22:40<01:47,  1.44it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  93%|▉| 1953/2106 [22:40<01:46,  1.44it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  93%|▉| 1954/2106 [22:40<01:45,  1.44it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  93%|▉| 1955/2106 [22:40<01:45,  1.44it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  93%|▉| 1956/2106 [22:40<01:44,  1.44it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  93%|▉| 1957/2106 [22:41<01:43,  1.44it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  93%|▉| 1958/2106 [22:41<01:42,  1.44it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  93%|▉| 1959/2106 [22:41<01:42,  1.44it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  93%|▉| 1960/2106 [22:41<01:41,  1.44it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  93%|▉| 1961/2106 [22:42<01:40,  1.44it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  93%|▉| 1962/2106 [22:42<01:39,  1.44it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  93%|▉| 1963/2106 [22:42<01:39,  1.44it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  93%|▉| 1964/2106 [22:42<01:38,  1.44it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  93%|▉| 1965/2106 [22:43<01:37,  1.44it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  93%|▉| 1966/2106 [22:43<01:37,  1.44it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  93%|▉| 1967/2106 [22:43<01:36,  1.44it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  93%|▉| 1968/2106 [22:43<01:35,  1.44it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  93%|▉| 1969/2106 [22:44<01:34,  1.44it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  94%|▉| 1970/2106 [22:44<01:34,  1.44it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  94%|▉| 1971/2106 [22:44<01:33,  1.44it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  94%|▉| 1972/2106 [22:44<01:32,  1.44it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  94%|▉| 1973/2106 [22:44<01:32,  1.45it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  94%|▉| 1974/2106 [22:45<01:31,  1.45it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  94%|▉| 1975/2106 [22:45<01:30,  1.45it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  94%|▉| 1976/2106 [22:45<01:29,  1.45it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  94%|▉| 1977/2106 [22:45<01:29,  1.45it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  94%|▉| 1978/2106 [22:46<01:28,  1.45it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  94%|▉| 1979/2106 [22:46<01:27,  1.45it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  94%|▉| 1980/2106 [22:46<01:26,  1.45it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  94%|▉| 1981/2106 [22:46<01:26,  1.45it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  94%|▉| 1982/2106 [22:47<01:25,  1.45it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  94%|▉| 1983/2106 [22:47<01:24,  1.45it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  94%|▉| 1984/2106 [22:47<01:24,  1.45it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  94%|▉| 1985/2106 [22:47<01:23,  1.45it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  94%|▉| 1986/2106 [22:48<01:22,  1.45it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  94%|▉| 1987/2106 [22:48<01:21,  1.45it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  94%|▉| 1988/2106 [22:48<01:21,  1.45it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  94%|▉| 1989/2106 [22:48<01:20,  1.45it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  94%|▉| 1990/2106 [22:49<01:19,  1.45it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  95%|▉| 1991/2106 [22:49<01:19,  1.45it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  95%|▉| 1992/2106 [22:49<01:18,  1.45it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  95%|▉| 1993/2106 [22:49<01:17,  1.46it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  95%|▉| 1994/2106 [22:49<01:16,  1.46it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  95%|▉| 1995/2106 [22:50<01:16,  1.46it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  95%|▉| 1996/2106 [22:50<01:15,  1.46it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  95%|▉| 1997/2106 [22:50<01:14,  1.46it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  95%|▉| 1998/2106 [22:50<01:14,  1.46it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  95%|▉| 1999/2106 [22:51<01:13,  1.46it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  95%|▉| 2000/2106 [22:51<01:12,  1.46it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  95%|▉| 2001/2106 [22:51<01:11,  1.46it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  95%|▉| 2002/2106 [22:51<01:11,  1.46it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  95%|▉| 2003/2106 [22:52<01:10,  1.46it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  95%|▉| 2004/2106 [22:52<01:09,  1.46it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  95%|▉| 2005/2106 [22:52<01:09,  1.46it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  95%|▉| 2006/2106 [22:52<01:08,  1.46it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  95%|▉| 2007/2106 [22:53<01:07,  1.46it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  95%|▉| 2008/2106 [22:53<01:07,  1.46it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  95%|▉| 2009/2106 [22:53<01:06,  1.46it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  95%|▉| 2010/2106 [22:53<01:05,  1.46it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  95%|▉| 2011/2106 [22:53<01:04,  1.46it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  96%|▉| 2012/2106 [22:54<01:04,  1.46it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  96%|▉| 2013/2106 [22:54<01:03,  1.46it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  96%|▉| 2014/2106 [22:54<01:02,  1.47it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  96%|▉| 2015/2106 [22:54<01:02,  1.47it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  96%|▉| 2016/2106 [22:55<01:01,  1.47it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  96%|▉| 2017/2106 [22:55<01:00,  1.47it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  96%|▉| 2018/2106 [22:55<00:59,  1.47it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  96%|▉| 2019/2106 [22:55<00:59,  1.47it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  96%|▉| 2020/2106 [22:56<00:58,  1.47it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  96%|▉| 2021/2106 [22:56<00:57,  1.47it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  96%|▉| 2022/2106 [22:56<00:57,  1.47it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  96%|▉| 2023/2106 [22:56<00:56,  1.47it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  96%|▉| 2024/2106 [22:57<00:55,  1.47it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  96%|▉| 2025/2106 [22:57<00:55,  1.47it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  96%|▉| 2026/2106 [22:57<00:54,  1.47it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  96%|▉| 2027/2106 [22:57<00:53,  1.47it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  96%|▉| 2028/2106 [22:58<00:53,  1.47it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  96%|▉| 2029/2106 [22:58<00:52,  1.47it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  96%|▉| 2030/2106 [22:58<00:51,  1.47it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  96%|▉| 2031/2106 [22:58<00:50,  1.47it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  96%|▉| 2032/2106 [22:58<00:50,  1.47it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  97%|▉| 2033/2106 [22:59<00:49,  1.47it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  97%|▉| 2034/2106 [22:59<00:48,  1.47it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  97%|▉| 2035/2106 [22:59<00:48,  1.48it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  97%|▉| 2036/2106 [22:59<00:47,  1.48it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  97%|▉| 2037/2106 [23:00<00:46,  1.48it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  97%|▉| 2038/2106 [23:00<00:46,  1.48it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  97%|▉| 2039/2106 [23:00<00:45,  1.48it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  97%|▉| 2040/2106 [23:00<00:44,  1.48it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  97%|▉| 2041/2106 [23:01<00:43,  1.48it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  97%|▉| 2042/2106 [23:01<00:43,  1.48it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  97%|▉| 2043/2106 [23:01<00:42,  1.48it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  97%|▉| 2044/2106 [23:01<00:41,  1.48it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  97%|▉| 2045/2106 [23:02<00:41,  1.48it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  97%|▉| 2046/2106 [23:02<00:40,  1.48it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  97%|▉| 2047/2106 [23:02<00:39,  1.48it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  97%|▉| 2048/2106 [23:02<00:39,  1.48it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  97%|▉| 2049/2106 [23:02<00:38,  1.48it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  97%|▉| 2050/2106 [23:03<00:37,  1.48it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  97%|▉| 2051/2106 [23:03<00:37,  1.48it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  97%|▉| 2052/2106 [23:03<00:36,  1.48it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  97%|▉| 2053/2106 [23:03<00:35,  1.48it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  98%|▉| 2054/2106 [23:04<00:35,  1.48it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  98%|▉| 2055/2106 [23:04<00:34,  1.48it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  98%|▉| 2056/2106 [23:04<00:33,  1.48it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  98%|▉| 2057/2106 [23:04<00:32,  1.49it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  98%|▉| 2058/2106 [23:05<00:32,  1.49it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  98%|▉| 2059/2106 [23:05<00:31,  1.49it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  98%|▉| 2060/2106 [23:05<00:30,  1.49it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  98%|▉| 2061/2106 [23:05<00:30,  1.49it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  98%|▉| 2062/2106 [23:06<00:29,  1.49it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  98%|▉| 2063/2106 [23:06<00:28,  1.49it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  98%|▉| 2064/2106 [23:06<00:28,  1.49it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  98%|▉| 2065/2106 [23:06<00:27,  1.49it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  98%|▉| 2066/2106 [23:06<00:26,  1.49it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  98%|▉| 2067/2106 [23:07<00:26,  1.49it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  98%|▉| 2068/2106 [23:07<00:25,  1.49it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  98%|▉| 2069/2106 [23:07<00:24,  1.49it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  98%|▉| 2070/2106 [23:07<00:24,  1.49it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  98%|▉| 2071/2106 [23:08<00:23,  1.49it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  98%|▉| 2072/2106 [23:08<00:22,  1.49it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  98%|▉| 2073/2106 [23:08<00:22,  1.49it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  98%|▉| 2074/2106 [23:08<00:21,  1.49it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  99%|▉| 2075/2106 [23:09<00:20,  1.49it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  99%|▉| 2076/2106 [23:09<00:20,  1.49it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  99%|▉| 2077/2106 [23:09<00:19,  1.49it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  99%|▉| 2078/2106 [23:09<00:18,  1.50it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  99%|▉| 2079/2106 [23:10<00:18,  1.50it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  99%|▉| 2080/2106 [23:10<00:17,  1.50it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  99%|▉| 2081/2106 [23:10<00:16,  1.50it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  99%|▉| 2082/2106 [23:10<00:16,  1.50it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  99%|▉| 2083/2106 [23:11<00:15,  1.50it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  99%|▉| 2084/2106 [23:11<00:14,  1.50it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  99%|▉| 2085/2106 [23:11<00:14,  1.50it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  99%|▉| 2086/2106 [23:11<00:13,  1.50it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  99%|▉| 2087/2106 [23:11<00:12,  1.50it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  99%|▉| 2088/2106 [23:12<00:12,  1.50it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  99%|▉| 2089/2106 [23:12<00:11,  1.50it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  99%|▉| 2090/2106 [23:12<00:10,  1.50it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  99%|▉| 2091/2106 [23:12<00:09,  1.50it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  99%|▉| 2092/2106 [23:13<00:09,  1.50it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  99%|▉| 2093/2106 [23:13<00:08,  1.50it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  99%|▉| 2094/2106 [23:13<00:07,  1.50it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1:  99%|▉| 2095/2106 [23:13<00:07,  1.50it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1: 100%|▉| 2096/2106 [23:14<00:06,  1.50it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1: 100%|▉| 2097/2106 [23:14<00:05,  1.50it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1: 100%|▉| 2098/2106 [23:14<00:05,  1.50it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1: 100%|▉| 2099/2106 [23:14<00:04,  1.50it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1: 100%|▉| 2100/2106 [23:15<00:03,  1.51it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1: 100%|▉| 2101/2106 [23:15<00:03,  1.51it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1: 100%|▉| 2102/2106 [23:15<00:02,  1.51it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1: 100%|▉| 2103/2106 [23:15<00:01,  1.51it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1: 100%|▉| 2104/2106 [23:15<00:01,  1.51it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1: 100%|▉| 2105/2106 [23:16<00:00,  1.51it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 1: 100%|█| 2106/2106 [23:16<00:00,  1.51it/s, loss=0.105, v_num=1, train_l\u001b[A\n",
      "Epoch 2:  90%|▉| 1895/2106 [22:28<02:30,  1.41it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                       | 0/211 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                          | 0/211 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  90%|▉| 1896/2106 [22:28<02:29,  1.41it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  90%|▉| 1897/2106 [22:28<02:28,  1.41it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  90%|▉| 1898/2106 [22:28<02:27,  1.41it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  90%|▉| 1899/2106 [22:29<02:27,  1.41it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  90%|▉| 1900/2106 [22:29<02:26,  1.41it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  90%|▉| 1901/2106 [22:29<02:25,  1.41it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  90%|▉| 1902/2106 [22:29<02:24,  1.41it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  90%|▉| 1903/2106 [22:30<02:24,  1.41it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  90%|▉| 1904/2106 [22:30<02:23,  1.41it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  90%|▉| 1905/2106 [22:30<02:22,  1.41it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  91%|▉| 1906/2106 [22:30<02:21,  1.41it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  91%|▉| 1907/2106 [22:31<02:20,  1.41it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  91%|▉| 1908/2106 [22:31<02:20,  1.41it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  91%|▉| 1909/2106 [22:31<02:19,  1.41it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  91%|▉| 1910/2106 [22:31<02:18,  1.41it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  91%|▉| 1911/2106 [22:32<02:17,  1.41it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  91%|▉| 1912/2106 [22:32<02:17,  1.41it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  91%|▉| 1913/2106 [22:32<02:16,  1.41it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  91%|▉| 1914/2106 [22:32<02:15,  1.41it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  91%|▉| 1915/2106 [22:33<02:14,  1.42it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  91%|▉| 1916/2106 [22:33<02:14,  1.42it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  91%|▉| 1917/2106 [22:33<02:13,  1.42it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  91%|▉| 1918/2106 [22:33<02:12,  1.42it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  91%|▉| 1919/2106 [22:33<02:11,  1.42it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  91%|▉| 1920/2106 [22:34<02:11,  1.42it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  91%|▉| 1921/2106 [22:34<02:10,  1.42it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  91%|▉| 1922/2106 [22:34<02:09,  1.42it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  91%|▉| 1923/2106 [22:34<02:08,  1.42it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  91%|▉| 1924/2106 [22:35<02:08,  1.42it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  91%|▉| 1925/2106 [22:35<02:07,  1.42it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  91%|▉| 1926/2106 [22:35<02:06,  1.42it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  92%|▉| 1927/2106 [22:35<02:05,  1.42it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  92%|▉| 1928/2106 [22:36<02:05,  1.42it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  92%|▉| 1929/2106 [22:36<02:04,  1.42it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  92%|▉| 1930/2106 [22:36<02:03,  1.42it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  92%|▉| 1931/2106 [22:36<02:02,  1.42it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  92%|▉| 1932/2106 [22:37<02:02,  1.42it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  92%|▉| 1933/2106 [22:37<02:01,  1.42it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  92%|▉| 1934/2106 [22:37<02:00,  1.42it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  92%|▉| 1935/2106 [22:37<01:59,  1.43it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  92%|▉| 1936/2106 [22:37<01:59,  1.43it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  92%|▉| 1937/2106 [22:38<01:58,  1.43it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  92%|▉| 1938/2106 [22:38<01:57,  1.43it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  92%|▉| 1939/2106 [22:38<01:57,  1.43it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  92%|▉| 1940/2106 [22:38<01:56,  1.43it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  92%|▉| 1941/2106 [22:39<01:55,  1.43it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  92%|▉| 1942/2106 [22:39<01:54,  1.43it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  92%|▉| 1943/2106 [22:39<01:54,  1.43it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  92%|▉| 1944/2106 [22:39<01:53,  1.43it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  92%|▉| 1945/2106 [22:40<01:52,  1.43it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  92%|▉| 1946/2106 [22:40<01:51,  1.43it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  92%|▉| 1947/2106 [22:40<01:51,  1.43it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  92%|▉| 1948/2106 [22:40<01:50,  1.43it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  93%|▉| 1949/2106 [22:41<01:49,  1.43it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  93%|▉| 1950/2106 [22:41<01:48,  1.43it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  93%|▉| 1951/2106 [22:41<01:48,  1.43it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  93%|▉| 1952/2106 [22:41<01:47,  1.43it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  93%|▉| 1953/2106 [22:42<01:46,  1.43it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  93%|▉| 1954/2106 [22:42<01:45,  1.43it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  93%|▉| 1955/2106 [22:42<01:45,  1.43it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  93%|▉| 1956/2106 [22:42<01:44,  1.44it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  93%|▉| 1957/2106 [22:42<01:43,  1.44it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  93%|▉| 1958/2106 [22:43<01:43,  1.44it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  93%|▉| 1959/2106 [22:43<01:42,  1.44it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  93%|▉| 1960/2106 [22:43<01:41,  1.44it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  93%|▉| 1961/2106 [22:43<01:40,  1.44it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  93%|▉| 1962/2106 [22:44<01:40,  1.44it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  93%|▉| 1963/2106 [22:44<01:39,  1.44it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  93%|▉| 1964/2106 [22:44<01:38,  1.44it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  93%|▉| 1965/2106 [22:44<01:37,  1.44it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  93%|▉| 1966/2106 [22:45<01:37,  1.44it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  93%|▉| 1967/2106 [22:45<01:36,  1.44it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  93%|▉| 1968/2106 [22:45<01:35,  1.44it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  93%|▉| 1969/2106 [22:45<01:35,  1.44it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  94%|▉| 1970/2106 [22:46<01:34,  1.44it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  94%|▉| 1971/2106 [22:46<01:33,  1.44it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  94%|▉| 1972/2106 [22:46<01:32,  1.44it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  94%|▉| 1973/2106 [22:46<01:32,  1.44it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  94%|▉| 1974/2106 [22:46<01:31,  1.44it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  94%|▉| 1975/2106 [22:47<01:30,  1.44it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  94%|▉| 1976/2106 [22:47<01:29,  1.45it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  94%|▉| 1977/2106 [22:47<01:29,  1.45it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  94%|▉| 1978/2106 [22:47<01:28,  1.45it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  94%|▉| 1979/2106 [22:48<01:27,  1.45it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  94%|▉| 1980/2106 [22:48<01:27,  1.45it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  94%|▉| 1981/2106 [22:48<01:26,  1.45it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  94%|▉| 1982/2106 [22:48<01:25,  1.45it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  94%|▉| 1983/2106 [22:49<01:24,  1.45it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  94%|▉| 1984/2106 [22:49<01:24,  1.45it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  94%|▉| 1985/2106 [22:49<01:23,  1.45it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  94%|▉| 1986/2106 [22:49<01:22,  1.45it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  94%|▉| 1987/2106 [22:50<01:22,  1.45it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  94%|▉| 1988/2106 [22:50<01:21,  1.45it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  94%|▉| 1989/2106 [22:50<01:20,  1.45it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  94%|▉| 1990/2106 [22:50<01:19,  1.45it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  95%|▉| 1991/2106 [22:51<01:19,  1.45it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  95%|▉| 1992/2106 [22:51<01:18,  1.45it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  95%|▉| 1993/2106 [22:51<01:17,  1.45it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  95%|▉| 1994/2106 [22:51<01:17,  1.45it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  95%|▉| 1995/2106 [22:51<01:16,  1.45it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  95%|▉| 1996/2106 [22:52<01:15,  1.45it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  95%|▉| 1997/2106 [22:52<01:14,  1.46it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  95%|▉| 1998/2106 [22:52<01:14,  1.46it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  95%|▉| 1999/2106 [22:52<01:13,  1.46it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  95%|▉| 2000/2106 [22:53<01:12,  1.46it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  95%|▉| 2001/2106 [22:53<01:12,  1.46it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  95%|▉| 2002/2106 [22:53<01:11,  1.46it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  95%|▉| 2003/2106 [22:53<01:10,  1.46it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  95%|▉| 2004/2106 [22:54<01:09,  1.46it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  95%|▉| 2005/2106 [22:54<01:09,  1.46it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  95%|▉| 2006/2106 [22:54<01:08,  1.46it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  95%|▉| 2007/2106 [22:54<01:07,  1.46it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  95%|▉| 2008/2106 [22:55<01:07,  1.46it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  95%|▉| 2009/2106 [22:55<01:06,  1.46it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  95%|▉| 2010/2106 [22:55<01:05,  1.46it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  95%|▉| 2011/2106 [22:55<01:04,  1.46it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  96%|▉| 2012/2106 [22:55<01:04,  1.46it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  96%|▉| 2013/2106 [22:56<01:03,  1.46it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  96%|▉| 2014/2106 [22:56<01:02,  1.46it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  96%|▉| 2015/2106 [22:56<01:02,  1.46it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  96%|▉| 2016/2106 [22:56<01:01,  1.46it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  96%|▉| 2017/2106 [22:57<01:00,  1.46it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  96%|▉| 2018/2106 [22:57<01:00,  1.47it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  96%|▉| 2019/2106 [22:57<00:59,  1.47it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  96%|▉| 2020/2106 [22:57<00:58,  1.47it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  96%|▉| 2021/2106 [22:58<00:57,  1.47it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  96%|▉| 2022/2106 [22:58<00:57,  1.47it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  96%|▉| 2023/2106 [22:58<00:56,  1.47it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  96%|▉| 2024/2106 [22:58<00:55,  1.47it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  96%|▉| 2025/2106 [22:59<00:55,  1.47it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  96%|▉| 2026/2106 [22:59<00:54,  1.47it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  96%|▉| 2027/2106 [22:59<00:53,  1.47it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  96%|▉| 2028/2106 [22:59<00:53,  1.47it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  96%|▉| 2029/2106 [22:59<00:52,  1.47it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  96%|▉| 2030/2106 [23:00<00:51,  1.47it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  96%|▉| 2031/2106 [23:00<00:50,  1.47it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  96%|▉| 2032/2106 [23:00<00:50,  1.47it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  97%|▉| 2033/2106 [23:00<00:49,  1.47it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  97%|▉| 2034/2106 [23:01<00:48,  1.47it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  97%|▉| 2035/2106 [23:01<00:48,  1.47it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  97%|▉| 2036/2106 [23:01<00:47,  1.47it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  97%|▉| 2037/2106 [23:01<00:46,  1.47it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  97%|▉| 2038/2106 [23:02<00:46,  1.47it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  97%|▉| 2039/2106 [23:02<00:45,  1.48it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  97%|▉| 2040/2106 [23:02<00:44,  1.48it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  97%|▉| 2041/2106 [23:02<00:44,  1.48it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  97%|▉| 2042/2106 [23:03<00:43,  1.48it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  97%|▉| 2043/2106 [23:03<00:42,  1.48it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  97%|▉| 2044/2106 [23:03<00:41,  1.48it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  97%|▉| 2045/2106 [23:03<00:41,  1.48it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  97%|▉| 2046/2106 [23:04<00:40,  1.48it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  97%|▉| 2047/2106 [23:04<00:39,  1.48it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  97%|▉| 2048/2106 [23:04<00:39,  1.48it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  97%|▉| 2049/2106 [23:04<00:38,  1.48it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  97%|▉| 2050/2106 [23:04<00:37,  1.48it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  97%|▉| 2051/2106 [23:05<00:37,  1.48it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  97%|▉| 2052/2106 [23:05<00:36,  1.48it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  97%|▉| 2053/2106 [23:05<00:35,  1.48it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  98%|▉| 2054/2106 [23:05<00:35,  1.48it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  98%|▉| 2055/2106 [23:06<00:34,  1.48it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  98%|▉| 2056/2106 [23:06<00:33,  1.48it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  98%|▉| 2057/2106 [23:06<00:33,  1.48it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  98%|▉| 2058/2106 [23:06<00:32,  1.48it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  98%|▉| 2059/2106 [23:07<00:31,  1.48it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  98%|▉| 2060/2106 [23:07<00:30,  1.48it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  98%|▉| 2061/2106 [23:07<00:30,  1.49it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  98%|▉| 2062/2106 [23:07<00:29,  1.49it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  98%|▉| 2063/2106 [23:08<00:28,  1.49it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  98%|▉| 2064/2106 [23:08<00:28,  1.49it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  98%|▉| 2065/2106 [23:08<00:27,  1.49it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  98%|▉| 2066/2106 [23:08<00:26,  1.49it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  98%|▉| 2067/2106 [23:08<00:26,  1.49it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  98%|▉| 2068/2106 [23:09<00:25,  1.49it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  98%|▉| 2069/2106 [23:09<00:24,  1.49it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  98%|▉| 2070/2106 [23:09<00:24,  1.49it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  98%|▉| 2071/2106 [23:09<00:23,  1.49it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  98%|▉| 2072/2106 [23:10<00:22,  1.49it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  98%|▉| 2073/2106 [23:10<00:22,  1.49it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  98%|▉| 2074/2106 [23:10<00:21,  1.49it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  99%|▉| 2075/2106 [23:10<00:20,  1.49it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  99%|▉| 2076/2106 [23:11<00:20,  1.49it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  99%|▉| 2077/2106 [23:11<00:19,  1.49it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  99%|▉| 2078/2106 [23:11<00:18,  1.49it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  99%|▉| 2079/2106 [23:11<00:18,  1.49it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  99%|▉| 2080/2106 [23:12<00:17,  1.49it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  99%|▉| 2081/2106 [23:12<00:16,  1.49it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  99%|▉| 2082/2106 [23:12<00:16,  1.50it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  99%|▉| 2083/2106 [23:12<00:15,  1.50it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  99%|▉| 2084/2106 [23:13<00:14,  1.50it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  99%|▉| 2085/2106 [23:13<00:14,  1.50it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  99%|▉| 2086/2106 [23:13<00:13,  1.50it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  99%|▉| 2087/2106 [23:13<00:12,  1.50it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  99%|▉| 2088/2106 [23:13<00:12,  1.50it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  99%|▉| 2089/2106 [23:14<00:11,  1.50it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  99%|▉| 2090/2106 [23:14<00:10,  1.50it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  99%|▉| 2091/2106 [23:14<00:10,  1.50it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  99%|▉| 2092/2106 [23:14<00:09,  1.50it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  99%|▉| 2093/2106 [23:15<00:08,  1.50it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  99%|▉| 2094/2106 [23:15<00:07,  1.50it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2:  99%|▉| 2095/2106 [23:15<00:07,  1.50it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2: 100%|▉| 2096/2106 [23:15<00:06,  1.50it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2: 100%|▉| 2097/2106 [23:16<00:05,  1.50it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2: 100%|▉| 2098/2106 [23:16<00:05,  1.50it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2: 100%|▉| 2099/2106 [23:16<00:04,  1.50it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2: 100%|▉| 2100/2106 [23:16<00:03,  1.50it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2: 100%|▉| 2101/2106 [23:17<00:03,  1.50it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2: 100%|▉| 2102/2106 [23:17<00:02,  1.50it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2: 100%|▉| 2103/2106 [23:17<00:01,  1.50it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2: 100%|▉| 2104/2106 [23:17<00:01,  1.51it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2: 100%|▉| 2105/2106 [23:17<00:00,  1.51it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2: 100%|█| 2106/2106 [23:18<00:00,  1.51it/s, loss=0.0642, v_num=1, train_\u001b[A\n",
      "Epoch 2: 100%|█| 2106/2106 [23:18<00:00,  1.51it/s, loss=0.0642, v_num=1, train_\u001b[A`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "Epoch 2: 100%|█| 2106/2106 [23:20<00:00,  1.50it/s, loss=0.0642, v_num=1, train_\n",
      "Loading cached split indices for dataset at /home/jovyan/GigaChat/censor_dasha/sst2.hf/train/cache-8d67fbfceca87106.arrow and /home/jovyan/GigaChat/censor_dasha/sst2.hf/train/cache-b41b7446ba1d3037.arrow\n",
      "You are using a CUDA device ('A100-PCIE-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Testing DataLoader 0: 100%|█████████████████████| 55/55 [00:12<00:00,  4.25it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "         test_f1            0.9174253374142508\n",
      "     test_precision         0.9270106012921483\n",
      "       test_recall          0.9140325937359258\n",
      "────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "! python finetune.py --glue_cls_task='sst2' --epochs=3 --lr=1e-5 --batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CsGkLJuFpHhj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fHJyAJmzpHk9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
